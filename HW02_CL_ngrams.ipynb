{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1. (5 баллов) \n",
    "В тетрадке реализована биграмная языковая модель (при генерации учитывается информация только о 1 предыдущем слове). Реализуйте триграмную модель и сгенерируйте несколько текстов. Сравните их с текстами, сгенерированными биграмной моделью. \n",
    "Можно использовать те же тексты, что в семинаре, или взять какой-то другой (на английском или русском языке).  \n",
    "\n",
    "Делать это задание будет легче после прочтения первых 7 страниц вот этой главы из Журафского - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dostoevsky = open('besy_dostoevsky.txt', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation += '«»—–…“”'\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.strip(punctuation) for word \n",
    "                       in text.lower().split()]\n",
    "    normalized_text = [word for word in normalized_text \n",
    "                       if word]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dostoevsky = normalize(dostoevsky)\n",
    "vocab_dostoevsky = Counter(norm_dostoevsky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.04316943693335454),\n",
       " ('в', 0.02362513917607762),\n",
       " ('не', 0.023520757117862254),\n",
       " ('что', 0.01769524415460474),\n",
       " ('я', 0.017178304437728647)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_dostoevsky = Counter({word:c/len(norm_dostoevsky) \n",
    "                             for word, c in vocab_dostoevsky.items()})\n",
    "probas_dostoevsky.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer (tokens, n=3):\n",
    "    ngrams = []\n",
    "    for i in range (0, len(tokens) - n + 1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dostoevsky = [['<start>'] + ['<start>'] + normalize(text) +\\\n",
    "                        ['<end>'] for text in sent_tokenize(dostoevsky)]\n",
    "sentences_dostoevsky = sentences_dostoevsky[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dostoevsky = Counter()\n",
    "bigrams_dostoevsky = Counter()\n",
    "trigrams_dostoevsky = Counter()\n",
    "\n",
    "for sentence in sentences_dostoevsky:\n",
    "  unigrams_dostoevsky.update(sentence)\n",
    "  bigrams_dostoevsky.update(ngrammer(sentence, n=2))\n",
    "  trigrams_dostoevsky.update(ngrammer(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посчитать условную вероятность мы можем поделить количество вхождений триграма на количество вхождений первых двух слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dostoevsky = np.zeros((len(bigrams_dostoevsky),\n",
    "                            len(unigrams_dostoevsky)))\n",
    "\n",
    "id2bigram_dostoevsky = list(bigrams_dostoevsky)\n",
    "bigram2id_dostoevsky = {word:i for i, word \n",
    "                        in enumerate(id2bigram_dostoevsky)}\n",
    "\n",
    "id2unigram_dostoevsky = list(unigrams_dostoevsky)\n",
    "unigram2id_dostoevsky = {word:i for i, word \n",
    "                        in enumerate(id2unigram_dostoevsky)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trigram in trigrams_dostoevsky:\n",
    "    word1, word2, word3 = trigram.split()\n",
    "    words1_2 = word1 + \" \" + word2\n",
    "    \n",
    "    matrix_dostoevsky[\n",
    "        bigram2id_dostoevsky[words1_2]\n",
    "    ][\n",
    "        unigram2id_dostoevsky[word3]\n",
    "#         условная вероятность генерируемого униграма после данного биграма\n",
    "    ] = (trigrams_dostoevsky[trigram]/bigrams_dostoevsky[words1_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, \n",
    "             n=100, start=['<start>','<start>']):\n",
    "    text = []\n",
    "    bigram = start\n",
    "    current_idx = word2id[' '.join(bigram)]\n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(\n",
    "#       выбирает среди столбиков (не строчек!)\n",
    "        matrix.shape[1],\n",
    "#       учитывает вероятности перехода стартового биграма с текущим\n",
    "#       индексом в каждый униграм (столбик)\n",
    "        p=matrix[current_idx]\n",
    "        )\n",
    "        \n",
    "        text.append(\n",
    "#           добавляет в текст униграм\n",
    "            id2word[chosen])\n",
    "#       если был выбран тег конца,  выбираем тег начала\n",
    "        if id2word[chosen] == '<end>':\n",
    "            bigram = start\n",
    "            current_idx = word2id[' '.join(bigram)]\n",
    "#       если любой другой униграм, то смотрим \n",
    "#       на последнее слово биграма, и из них стряпаем биграм\n",
    "#       для следующего предсказания\n",
    "        else:\n",
    "          bigram = [bigram[1], id2word[chosen]]\n",
    "          current_idx = word2id[' '.join(bigram)]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "у нас исписываются совсем даже тихо промолвил степан трофимович \n",
      " конечно вы на нас любуется ну ближе ближе \n",
      " оставим это фр \n",
      " сомнения нет что эти легендарные господа способны были ощущать и даже не притворено \n",
      " вместо высших назначений она вдруг радостно усмехнулась \n",
      " ну-с угодно вам выслушать \n",
      " варвара петровна была скромно и по-всегдашнему одета во всё черное так бессменно одевалась она в изумлении \n",
      " вспоминаю с улыбкой я не очень разговорчив изящен без изысканности удивительно скромен и в два часа опять нет \n",
      " вы там каким-нибудь шефом меня представили \n",
      " dieu qui est l&#224;-haut et qui est\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix=matrix_dostoevsky,\n",
    "              id2word=id2unigram_dostoevsky,\n",
    "              word2id=bigram2id_dostoevsky).replace('<end>', '\\n'))\n",
    "# о, французский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2. (5 баллов) \n",
    "При помощи gensim.models.Phrases реализуйте byte-pair-encoding, про который говорилось на первом семинаре (https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/Preprocessing.ipynb) \n",
    "А именно 1) возьмите любой текст; разбейте его на предложения, а каждое предложение разбейте на отдельные символы (не потеряйте пробелы) 2) обучите gensim.models.Phrases на полученных символьных предложениях 3) примените полученный нграммер к этим символьным предложениям 4) повторите 2 и 3 N количество раз, чтобы начали получаться целые слова или фразы\n",
    "Параметры в gensim.models.Phrases влияют на количество получаемых нграммов после каждого прохода, поэтому не забудьте их настроить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что, если добавить пробел в common_terms, слова склеиваются лучше. Но при этом повисают соседние пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open('anna_karenina.txt', encoding='utf-8').read()\n",
    "txt_sent = nltk.sent_tokenize(txt, 'english')\n",
    "txt_sent = [' '.join(normalize(sent)) for sent in txt_sent]\n",
    "txt_sent = [[char for char in sent] for sent in txt_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ну', ' ', 'та', 'к э', 'то', 'й м', 'ар', 'ки', ' ', 'к у', 'ст', 'ри', 'ца', 'м п', 'од', 'ай', ' ', 'а т', 'ам', ' ', 'ви', 'дн', 'о б', 'уд', 'ет']\n",
      "['ну та', 'к это', 'й мар', 'ки к у', 'стри', 'цам п', 'одай', ' ', 'а там', ' ', 'видн', 'о буд', 'ет']\n",
      "['ну так это', 'й марки к у', 'стрицам п', 'одай а там', ' ', 'видно буд', 'ет']\n",
      "['ну так этой марки к у', 'стрицам подай а там', ' ', 'видно будет']\n"
     ]
    }
   ],
   "source": [
    "def glue(stopchars=[]):\n",
    "    phrasesmodel = gensim.models.Phrases(txt_sent, min_count=1,\n",
    "                               threshold=-1,\n",
    "                               delimiter=b'',\n",
    "                            #    Write logs every progress_per sentence\n",
    "                            #    progress_per=,\n",
    "                            #    Score for given bi-gram, in the range -1 to 1\n",
    "                               scoring='npmi',\n",
    "                            #    Stopwords\n",
    "                            #    common_terms=stopwords.words('russian')\n",
    "                                common_terms=[' ']+stopchars\n",
    "                                )\n",
    "    print(list(phrasesmodel[txt_sent])[800])\n",
    "\n",
    "    phrasesmodel2 = gensim.models.Phrases(phrasesmodel[txt_sent], min_count=1,\n",
    "                                          delimiter=b'', threshold=-1, \n",
    "                                          scoring='npmi', common_terms=[' ']+stopchars)\n",
    "    print(list(phrasesmodel2[phrasesmodel[txt_sent]])[800])\n",
    "\n",
    "    phrasesmodel3 = gensim.models.Phrases(phrasesmodel2[phrasesmodel[txt_sent]], \n",
    "                                          min_count=1, delimiter=b'', threshold=-1, \n",
    "                                          scoring='npmi', common_terms=[' ']+stopchars)\n",
    "    print(list(phrasesmodel3[phrasesmodel2[phrasesmodel[txt_sent]]])[800])\n",
    "\n",
    "    phrasesmodel4 = gensim.models.Phrases(phrasesmodel3[phrasesmodel2[phrasesmodel[txt_sent]]], \n",
    "                                          min_count=1, threshold=-1, scoring='npmi', \n",
    "                                          delimiter=b'', common_terms=[' ']+stopchars)\n",
    "    print(list(phrasesmodel4[phrasesmodel3[phrasesmodel2[phrasesmodel[txt_sent]]]])[800])\n",
    "\n",
    "glue(stopchars=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ну т', 'а', 'к', ' ', 'эт', 'о', 'й м', 'а', 'рки к уст', 'риц', 'а', 'м п', 'о', 'дай', ' ', 'а', ' ', 'там', ' ', 'в', 'и', 'дн', 'о', ' ', 'буд', 'ет']\n",
      "['ну так эт', 'о', 'й марки к уст', 'рицам п', 'о', 'дай а там', ' ', 'в', 'и', 'дно буд', 'ет']\n",
      "['ну так этой марки к уст', 'рицам подай а там', ' ', 'в', 'и', 'дно будет']\n",
      "['ну так этой марки к устрицам подай а там', ' ', 'в', 'и', 'дно будет']\n"
     ]
    }
   ],
   "source": [
    "glue(stopchars=[x[0] for x in probas_dostoevsky.most_common(200) if len(x[0])==1])\n",
    "# не очень хорошая оказалась идея, из-за неё развалилось ВИДНО\n",
    "# но склеиваться стало быстрее"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
