{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ира\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import zipfile\n",
    "from pymystem3 import Mystem\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "from string import punctuation\n",
    "punctuation += '«»—…“”*№–'\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('russian'))\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    words = [word.strip(punctuation) \n",
    "        for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form\n",
    "            for word in words if word\n",
    "            and word not in stops]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    words = [word.strip(punctuation) \n",
    "        for word in text.lower().split()]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Возьмите данные соревнования по определению перефразирования - http://paraphraser.ru/download/get?file_id=1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(\n",
    "    open('paraphraser/paraphrases.xml', \n",
    "         'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'text_1':texts_1,\n",
    "    'text_2':texts_2,\n",
    "    'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0</td>\n",
       "      <td>право полицейский проникновение жилища решить ...</td>\n",
       "      <td>правило внесудебный проникновение полицейский ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  Право полицейских на проникновение в жилище ре...   \n",
       "\n",
       "                                              text_2 label  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...     0   \n",
       "1  Правила внесудебного проникновения полицейских...     0   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилища решить ...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  полиция мочь разрешить стрелять хулиган травма...  \n",
       "1  правило внесудебный проникновение полицейский ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Задание делится на 2 части:*\n",
    "\n",
    "*1) Векторизуйте тексты с помощью Word2vec модели, обученной самостоятельно, и с помощью модели, взятой с rusvectores (любой). Обучите 2 модели по определению перефразирования на получившихся векторах и проверьте, что работает лучше.* \n",
    "\n",
    "*Word2Vec нужно обучить на отдельном корпусе (не на парафразах). Можно взять данные из семинара или любые другие.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Сравнение модели, обученной самостоятельно, и модели с русвекторес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель, обученная самостоятельно (на корпусе из кругосвета)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем и нормализуем корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['абай василий васо иван 1900–2001 русский лингвист родиться 2 15 декабрь 1900 с.коби тифлисский губерния ныне грузия 1925 окончить факультет общественный наука ленинградский университет 1928 аспирантура 1928–1930 сотрудник кавказский историко-археологический институт ан ссср 1930 полвека работать яфетический институт затем институт язык мышление институт языкознание ан ссср ленинград 1950 москва доктор филологический наука 1962 профессор 1969 лауреат государственный премия ссср 1981 почётный член азиатский королевский общество великобритания ирландия 1966 член-корреспондент финно-угорский общество хельсинки 1973 умереть абай москва 18 март 2001',\n",
       " 'также тема']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = 'corpus_hum.txt'\n",
    "with open(fpath, 'r', encoding='utf-8') as f:\n",
    "    corpus_norm = [normalize(text) for text in f.readlines()]\n",
    "corpus_norm = [text for text in corpus_norm if text]\n",
    "corpus_norm[:2]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем ворд2век"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model = gensim.models.Word2Vec([text.split() \n",
    "                             for text\n",
    "                             in corpus_norm],\n",
    "                             size=50,\n",
    "                             sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция из семинара которая эмбеддит текст\n",
    "def get_embedding(text, model):\n",
    "    dim=model.vector_size\n",
    "    text = text.split()\n",
    "    \n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v*(words[word] #счетчик слова\n",
    "                           /total)\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "#     проверяем, есть ли вообще ненулевые\n",
    "#     значения в этой матрице\n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддим пары текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v_model)\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Склеиваем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_w2v = np.concatenate([X_text_1_w2v,\n",
    "                            X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем рэндом форест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=7, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=15, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=7,\n",
    "                            min_samples_leaf=15,\n",
    "                            class_weight='balanced')\n",
    "clf.fit(X_text_w2v, data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ВАЖНО: Оценивать модели нужно с помощью кросс-валидации! Метрика - f1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оцениваем кросс-валидацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation weighted F1-score: 0.454\n",
      "Mean validation micro F1-score: 0.447\n",
      "Mean validation macro F1-score: 0.439\n"
     ]
    }
   ],
   "source": [
    "for score_type in 'weighted micro macro'.split():\n",
    "    print ('Mean validation {} F1-score:'.format(score_type), \n",
    "        round(\n",
    "            float(np.mean(cross_val_score(\n",
    "            estimator=clf,\n",
    "            X=X_text_w2v,\n",
    "            y=data['label'],   \n",
    "            scoring='f1_{}'.format(score_type),\n",
    "            cv=5, \n",
    "            n_jobs=-1))),\n",
    "            3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель с русвекторес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель скачали заранее. Это скипграм, обученный на тайге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('185.zip', 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    rv_model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код из семинара и из туториала русвекторес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "mapping_file='ru-rnc.map.txt'\n",
    "with open(mapping_file, 'r', encoding='utf-8') as f:\n",
    "    for pair in f.readlines():\n",
    "        pair = re.sub('\\s+', ' ', pair, flags=re.U).split(' ')\n",
    "#         if len(pair) > 1:\n",
    "        mapping[pair[0]] = pair[1]\n",
    "# print(mapping)\n",
    "\n",
    "m = Mystem()\n",
    "def normalize_mystem(text):\n",
    "    tokens = []\n",
    "    norm_words = m.analyze(text)\n",
    "    for norm_word in norm_words:\n",
    "        if 'analysis' not in norm_word:\n",
    "            continue\n",
    "            \n",
    "        if not len(norm_word['analysis']):\n",
    "            lemma = norm_word['text']\n",
    "            pos = 'UNKN'\n",
    "        else:\n",
    "            lemma = norm_word[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = norm_word[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "        pos = mapping[pos]\n",
    "        tokens.append(lemma+'_'+pos)\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_1', 'text_2', 'label', 'text_1_norm', 'text_2_norm'], dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще эта функция нормализует"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'текст_NOUN нужно_ADV передавать_VERB функция_NOUN в_ADP вид_NOUN строка_NOUN'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_mystem(text='Текст нужно передать функции в виде строки!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но применять мы её, конечно, не будем (это очень долго). Спасибо, что применили за нас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data['text_1_upos'] = data['text_1'].apply(normalize_mystem)\n",
    "# data['text_2_upos'] = data['text_2'].apply(normalize_mystem)\n",
    "data_upos = pd.read_csv('data_paraphraser_norm.csv')\n",
    "data['text_1_upos'] = data_upos['text_1_norm']\n",
    "data['text_2_upos'] = data_upos['text_2_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь делаем то же самое для предобученной кем-то модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_text_1_w2v = np.zeros((len(data['text_1_upos']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_upos']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_upos'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, rv_model)\n",
    "for i, text in enumerate(data['text_2_upos'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, rv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_w2v = np.concatenate([X_text_1_w2v,\n",
    "                            X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=7, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=15, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=7,\n",
    "                            min_samples_leaf=15,\n",
    "                            class_weight='balanced')\n",
    "clf.fit(X_text_w2v, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation weighted F1-score: 0.45\n",
      "Mean validation micro F1-score: 0.449\n",
      "Mean validation macro F1-score: 0.431\n"
     ]
    }
   ],
   "source": [
    "for score_type in 'weighted micro macro'.split():\n",
    "    print ('Mean validation {} F1-score:'.format(score_type), \n",
    "        round(\n",
    "            float(np.mean(cross_val_score(\n",
    "            estimator=clf,\n",
    "            X=X_text_w2v,\n",
    "            y=data['label'],   \n",
    "            scoring='f1_{}'.format(score_type),\n",
    "            cv=5, \n",
    "            n_jobs=-1))),\n",
    "            3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кругосветный w2v:\n",
    "\n",
    "Mean validation weighted F1-score: 0.454\n",
    "\n",
    "Mean validation micro F1-score: 0.447\n",
    "\n",
    "Mean validation macro F1-score: 0.439\n",
    "\n",
    "Если сравнивать с моделью на кругосвете, разницы не видно вообще. Видимо потому что обе эти модели взяты с потолка и не учитывают специфику текстов парафразов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Обучение модели на косинусных близостях пар текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2) Преобразуйте тексты в векторы в каждой паре 5 методами  - SVD, NMF, Word2Vec (свой и  русвекторовский), Fastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# наш w2v\n",
    "w2v_embeddings = list(zip([get_embedding(text, w2v_model) \n",
    "                  for text in data['text_1_norm'].values], \n",
    "                          [get_embedding(text, w2v_model) \n",
    "                  for text in data['text_2_norm'].values]))\n",
    "data['w2v_cosine_similarity'] = [cosine_similarity(\n",
    "    np.atleast_2d(w2v_embeddings[i][0]), \n",
    "    np.atleast_2d(w2v_embeddings[i][1]))[0,0] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим на кругосвете фасттекст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ft_model = gensim.models.FastText([text.split()\n",
    "                                for text \n",
    "                                in corpus_norm],\n",
    "                                size=50,\n",
    "# Minimum length of char n-grams \n",
    "# to be used for training word representations\n",
    "                                min_n=4,\n",
    "# Max length of char ngrams \n",
    "                                max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fasttext\n",
    "ft_embeddings = list(zip([get_embedding(text, ft_model) \n",
    "                  for text in data['text_1_norm'].values], \n",
    "                          [get_embedding(text, ft_model) \n",
    "                  for text in data['text_2_norm'].values]))\n",
    "data['ft_cosine_similarity'] = [cosine_similarity(\n",
    "    np.atleast_2d(ft_embeddings[i][0]), \n",
    "    np.atleast_2d(ft_embeddings[i][1]))[0,0] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# skipgram с rusvectores\n",
    "rv_embeddings = list(zip([get_embedding(text, rv_model) \n",
    "                  for text in data['text_1_upos'].values], \n",
    "                          [get_embedding(text, rv_model) \n",
    "                  for text in data['text_2_upos'].values]))\n",
    "data['rv_cosine_similarity'] = [cosine_similarity(\n",
    "    np.atleast_2d(rv_embeddings[i][0]), \n",
    "    np.atleast_2d(rv_embeddings[i][1]))[0,0] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально в словаре векторайзера было 1000 фич, но NMF очень медленно обучался. Так что SVD обучили на 1000 фич, а NMF кое-как на 200 (зато за полчаса, а не за сто лет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.4, max_features=200,\n",
       "                min_df=3, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv = TfidfVectorizer(min_df=3, max_df=0.4,\n",
    "#                     max_features=1000\n",
    "                    max_features=200)\n",
    "tv.fit(pd.concat([data['text_1_norm'],\n",
    "                 data['text_2_norm']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(200)\n",
    "\n",
    "data['svd_cosine_similarity'] = [cosine_similarity(\n",
    "    np.atleast_2d(svd.fit_transform(\n",
    "        tv.transform(data['text_1_norm']))[i]), \n",
    "    np.atleast_2d(svd.fit_transform(\n",
    "        tv.transform(data['text_2_norm']))[i]))[0,0] \n",
    "                                 for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом моменте мы пошли в колаб, но это не принесло результата, пришлось просто обучить из рук вон плохо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('5cosine_similarities.csv', encoding='utf-8')\n",
    "data = pd.read_csv('5cosine_similarities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=3, \n",
    "#           ‘random’ | ‘nndsvd’ | ‘nndsvda’\n",
    "#           init='nndsvdar',\n",
    "          init=None,\n",
    "          solver='cd', \n",
    "          beta_loss='frobenius', \n",
    "#           Tolerance of the stopping condition\n",
    "#           tol=0.0001,\n",
    "          tol=0.1, \n",
    "#           max_iter=200,\n",
    "          max_iter=100, \n",
    "          random_state=1, \n",
    "#           Constant that multiplies the regularization terms\n",
    "          alpha=0.0, \n",
    "#           For l1_ratio = 0 the penalty is \n",
    "#           an elementwise L2 penalty (aka Frobenius Norm). \n",
    "          l1_ratio=0.0, \n",
    "#           verbose=1, \n",
    "#           If true, randomize the order of coordinates in the CD solver.\n",
    "          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time Wall time: 34min 48s\n",
    "\n",
    "data['nmf_cosine_similarity'] = [cosine_similarity(\n",
    "    np.atleast_2d(nmf.fit_transform(\n",
    "        tv.transform(data['text_1_norm']))[i]), \n",
    "    np.atleast_2d(nmf.fit_transform(\n",
    "        tv.transform(data['text_2_norm']))[i]))[0,0] \n",
    "                                 for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n",
      "105\n",
      "228\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "# она настолько плохая, что скорее посчитает парафразами всё кроме парафразов\n",
    "# вообще f1-micro отдельно этой модели около 30\n",
    "print(len(data[data['nmf_cosine_similarity']==1]))\n",
    "print(len(data[(data['nmf_cosine_similarity']==1) & (data['label']=='1')]))\n",
    "print(len(data[(data['nmf_cosine_similarity']==1) & (data['label']=='0')]))\n",
    "print(len(data[(data['nmf_cosine_similarity']==1) & (data['label']=='-1')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Постройте обучающую выборку из этих близостей . Обучите любую модель (Логрег, Рандом форест или что-то ещё) на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SVD и NMF применяйте к данным напрямую, а w2w и fastext обучите на отдельном корпусе (как в первой части).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = data[data.columns[7:]].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем рэндом форест на близостях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=7, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=15, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=7,\n",
    "                            min_samples_leaf=15,\n",
    "                            class_weight='balanced')\n",
    "clf.fit(X5, data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оцениваемся на кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation weighted F1-score: 0.54521\n",
      "Mean validation micro F1-score: 0.54904\n",
      "Mean validation macro F1-score: 0.54766\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for score_type in 'weighted micro macro'.split():\n",
    "    print ('Mean validation {} F1-score:'.format(score_type), \n",
    "        round(\n",
    "            float(np.mean(cross_val_score(\n",
    "            estimator=clf,\n",
    "#             X=train_X,\n",
    "            X=X5,\n",
    "#             y=train_y,\n",
    "            y=data['label'],   \n",
    "            scoring='f1_{}'.format(score_type),\n",
    "            cv=15, \n",
    "            n_jobs=-1))),\n",
    "            5))\n",
    "\n",
    "# Результаты на кругосветном w2v:\n",
    "# Mean validation weighted F1-score: 0.454\n",
    "# Mean validation micro F1-score: 0.447\n",
    "# Mean validation macro F1-score: 0.439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**На близостях получилось лучше, ура! Причём даже самая плохая модель, как кажется, внесла свой вклад (без неё, то есть на 4 близостях вместо 5, метрика была чуть хуже).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Всё ещё часть 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Попробуйте улучшить метрику, изменив параметры в методах векторизации.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# поменяли размер векторов\n",
    "w2v_model = gensim.models.Word2Vec([text.split() \n",
    "                             for text\n",
    "                             in corpus_norm],\n",
    "                             size=100,\n",
    "                             sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# поменяли размер векторов и увеличили длину нграмов\n",
    "ft_model = gensim.models.FastText([text.split()\n",
    "                                for text \n",
    "                                in corpus_norm],\n",
    "                                size=100,\n",
    "# Minimum length of char n-grams \n",
    "# to be used for training word representations\n",
    "                                min_n=5,\n",
    "# Max length of char ngrams \n",
    "                                max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ира\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fasttext\n",
    "ft_embeddings = list(zip([get_embedding(text, ft_model) \n",
    "                  for text in data['text_1_norm'].values], \n",
    "                          [get_embedding(text, ft_model) \n",
    "                  for text in data['text_2_norm'].values]))\n",
    "data['ft_cosine_similarity'] = [cosine_similarity(\n",
    "    np.atleast_2d(ft_embeddings[i][0]), \n",
    "    np.atleast_2d(ft_embeddings[i][1]))[0,0] for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.4, max_features=400,\n",
       "                min_df=3, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# поменяли количество фичей\n",
    "tv = TfidfVectorizer(min_df=3, max_df=0.4,\n",
    "#                     max_features=1000\n",
    "                    max_features=400)\n",
    "tv.fit(pd.concat([data['text_1_norm'],\n",
    "                 data['text_2_norm']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# поменяли размер векторов\n",
    "svd = TruncatedSVD(300)\n",
    "\n",
    "data['svd_cosine_similarity'] = [cosine_similarity(\n",
    "    np.atleast_2d(svd.fit_transform(\n",
    "        tv.transform(data['text_1_norm']))[i]), \n",
    "    np.atleast_2d(svd.fit_transform(\n",
    "        tv.transform(data['text_2_norm']))[i]))[0,0] \n",
    "                                 for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=10, #поменяли\n",
    "#           ‘random’ | ‘nndsvd’ | ‘nndsvda’\n",
    "          init='nndsvdar', #поменяли\n",
    "#           init=None,\n",
    "          solver='cd', \n",
    "          beta_loss='frobenius', \n",
    "#           Tolerance of the stopping condition\n",
    "          tol=0.01, #поменяли\n",
    "          max_iter=100, \n",
    "          random_state=1, \n",
    "#           Constant that multiplies the regularization terms\n",
    "          alpha=0.0, \n",
    "#           For l1_ratio = 0 the penalty is \n",
    "#           an elementwise L2 penalty (aka Frobenius Norm). \n",
    "          l1_ratio=0.0,  \n",
    "#           If true, randomize the order of coordinates in the CD solver.\n",
    "          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "data['nmf_cosine_similarity'] = [cosine_similarity(\n",
    "    np.atleast_2d(nmf.fit_transform(\n",
    "        tv.transform(data['text_1_norm']))[i]), \n",
    "    np.atleast_2d(nmf.fit_transform(\n",
    "        tv.transform(data['text_2_norm']))[i]))[0,0] \n",
    "                                 for i in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = data[data.columns[-5:]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=8, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=15, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                            max_depth=8, #увеличили\n",
    "                            min_samples_leaf=15,\n",
    "                            class_weight='balanced')\n",
    "clf.fit(X5, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation weighted F1-score: 0.55242\n",
      "Mean validation micro F1-score: 0.5572\n",
      "Mean validation macro F1-score: 0.54995\n",
      "Wall time: 40.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for score_type in 'weighted micro macro'.split():\n",
    "    print ('Mean validation {} F1-score:'.format(score_type), \n",
    "        round(\n",
    "            float(np.mean(cross_val_score(\n",
    "            estimator=clf,\n",
    "            X=X5,\n",
    "            y=data['label'],   \n",
    "            scoring='f1_{}'.format(score_type),\n",
    "            cv=15, \n",
    "            n_jobs=-1))),\n",
    "            5))\n",
    "\n",
    "# Micro f1 на близостях в первый раз\n",
    "# Mean validation micro F1-score: 0.54904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Незначительные улучшения метрики."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
